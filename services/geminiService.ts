import { GoogleGenAI } from "@google/genai";
import { GeneratedImage } from "../types";

// Initialize the client
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

/**
 * Converts a File object to a Base64 string.
 */
export const fileToGenerativePart = async (file: File): Promise<{ inlineData: { data: string; mimeType: string } }> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64String = (reader.result as string).split(',')[1];
      resolve({
        inlineData: {
          data: base64String,
          mimeType: file.type,
        },
      });
    };
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
};

/**
 * Edits an image using Gemini 2.5 Flash Image based on a text prompt.
 */
export const editPetImage = async (
  imageFile: File,
  prompt: string
): Promise<GeneratedImage> => {
  try {
    const imagePart = await fileToGenerativePart(imageFile);
    
    // Using the specific "nano banana" equivalent model for image tasks
    const model = 'gemini-2.0-flash-exp';
    
    const response = await ai.models.generateContent({
      model: model,
      contents: {
        parts: [
          imagePart,
          { text: prompt }
        ],
      },
    });

    // Iterate through parts to find the image output
    if (response.candidates && response.candidates[0].content.parts) {
      for (const part of response.candidates[0].content.parts) {
        if (part.inlineData) {
          return {
            mimeType: part.inlineData.mimeType || 'image/png',
            data: part.inlineData.data,
          };
        }
      }
    }
    
    throw new Error("No image generated by the model.");

  } catch (error) {
    console.error("Error editing image:", error);
    throw error;
  }
};
